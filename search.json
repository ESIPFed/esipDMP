[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "sections/standardization.html",
    "href": "sections/standardization.html",
    "title": "Standardization of Data and Metadata",
    "section": "",
    "text": "Use of widely accepted formats (e.g., CSV, JSON, HDF5 for data; XML, JSON-LD for metadata).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nField observation data will be stored in flat ASCII files, which can be read easily by different software packages. Observational and experimental data from processed samples will be stored mainly as spreadsheets. Imagery from the seafloor will be stored in original resolution (this will be the biggest data volume in this project). Metadata will be prepared in accordance with BCO-DMO conventions (i.e. using the BCO-DMO metadata forms) and will include detailed descriptions of collection and analysis procedures.",
    "crumbs": [
      "Standardization of Data and Metadata"
    ]
  },
  {
    "objectID": "sections/standardization.html#how-will-it-be-standardized",
    "href": "sections/standardization.html#how-will-it-be-standardized",
    "title": "Standardization of Data and Metadata",
    "section": "",
    "text": "Use of widely accepted formats (e.g., CSV, JSON, HDF5 for data; XML, JSON-LD for metadata).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nField observation data will be stored in flat ASCII files, which can be read easily by different software packages. Observational and experimental data from processed samples will be stored mainly as spreadsheets. Imagery from the seafloor will be stored in original resolution (this will be the biggest data volume in this project). Metadata will be prepared in accordance with BCO-DMO conventions (i.e. using the BCO-DMO metadata forms) and will include detailed descriptions of collection and analysis procedures.",
    "crumbs": [
      "Standardization of Data and Metadata"
    ]
  },
  {
    "objectID": "sections/standardization.html#naming-conventions-and-file-structures",
    "href": "sections/standardization.html#naming-conventions-and-file-structures",
    "title": "Standardization of Data and Metadata",
    "section": "Naming conventions and file structures",
    "text": "Naming conventions and file structures\n\nStandards\n\nAdherence to domain-specific standards (e.g., Darwin Core for biodiversity data, DICOM for medical imaging).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nSpecifically for animals: We will utilize the World Register of Marine Species (WoRMS) taxonomic classification. We will standardize to Darwin Core format to provide occurrence data to OBIS and GBIF. Specifically for microbes: Genetic sequence data will be prepared in accordance with the minimum information about a marker gene sequence (MIMARKS) and about a metagenome sequence (MIMS) developed by the Genomic Standards Consortium, with quality control screening of raw reads from 16S/18S rRNA sequencing and raw metagenomic sequences. Specifically for minerals: Rock samples will be assigned International Geo Sample Numbers (IGSN) through SESAR (System for Earth Sample Registration)\n\n\n\nCompliance with FAIR principles (Findable, Accessible, Interoperable, Reusable).\n\n\n\n\n\n\n\nLTER: Sosik\n\n\n\nTo contribute FAIR (Findable, Accessible, Interoperable, Reusable) data products to DataONE and other community repositories, our IM team uses non-proprietary data formats, standardizes metadata, and promotes the use of controlled vocabularies.\n\n\n\n\nQA/QC Methods\n\nImplementation of quality assurance and quality control processes.\nRegular audits and validation checks to ensure data integrity.\n\n\n\n\n\n\n\nBCO-DMO: Saito\n\n\n\nQuality flags will be assigned according to the ODS IODE Quality Flag scheme (IOC Manuals and Guides, 54, volume 3; http://www.iode.org/mg54_3.\n\n\n\n\n\n\n\n\nIOOS: SCCOOS\n\n\n\nFor sources that do not provide quality flags, the SCCOOS DMAC Sub-System runs QARTOD tests after ingesting observation data. Tests are run using the open-source ioos_qc library, which implements a suite of QARTOD tests as well as other quality control algorithms. The quality test code and test thresholds are documented and publicly available through the CalOOS Data Portal. Links to the ioos_qc methods used are available both within data charts and on sensor pages within the CalOOS Data Portal. Thresholds used for each test are also viewable on sensor pages and users are linked to the test code in GitHub.",
    "crumbs": [
      "Standardization of Data and Metadata"
    ]
  },
  {
    "objectID": "sections/outputs.html",
    "href": "sections/outputs.html",
    "title": "Research Outputs",
    "section": "",
    "text": "Research outputs include a variety of materials and data generated during the research process. These may include:\n\nSamples\n\nPhysical samples (e.g., biological, chemical, geological)\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nPhysical samples will include animals, rocks, and water column samples. Rocks and animal specimens for which we extract gut contents will be assigned unique sample identifiers, important for provenance for subsamples to be analyzed for microbes.\n\n\n\nDigital samples (e.g., scanned images, 3D models)\n\n\n\nData\n\nRaw data\nProcessed data\nMetadata\n\n\n\nCollections\n\nCurated sets of samples or data\n\n\n\nSoftware\n\nCode, scripts, or applications developed for analysis\n\n\n\nMaterials\n\nLab protocols, reagents, or experimental setups\n\n\n\nModels\n\nStatistical, mathematical, or computational models\n\n\n\n\n\n\n\nBCO-DMO: Bianchi\n\n\n\nOutput from model simulations, which will include both physical (T, S, velocities) and biogeochemical (tracer, rates, fluxes) variables. The raw output will be post-processed and analyzed for presentation at meetings and for publication in scientific journals. For model output from the main set of simulations (Table 1 in the proposal), standard model fields (e.g. physical and biogeochemical tracers, current velocities, and the main biogeochemical rates) at monthly resolution will be provided to, and made available through, BCO-DMO and NOAA NCEI in standard NetCDF format.\n\n\n\n\nDocuments/Posters/Pre-prints/Publications/Videos\n\nResearch papers, conference posters, pre-prints, and multimedia content\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nImagery at the seafloor will include NDSF vehicle imagery (e.g., 4K video) and PI-provided imagery (e.g., time-lapse still).\n\n\n\n\n\n\n\n\nBCO-DMO: Alexander\n\n\n\n\nType of Research Output\nResearch outputs include a variety of materials and data generated during the research process. These may include:\n\nCollections\n\nCurated sets of samples or data\n\nData\n\nRaw data\nProcessed data\nMetadata\n\nDocuments/Posters/Pre-prints/Publications/Videos\n\nResearch papers, conference posters, pre-prints, and multimedia content\n\ne.g.:\n\nMullineaux #2453\n\nImagery at the seafloor will include NDSF vehicle imagery (e.g., 4K video) and PI-provided imagery (e.g., time-lapse still).\n\n\n\n\nMaterials\n\nLab protocols, reagents, or experimental setups\n\nModels\n\nStatistical, mathematical, or computational models\n\ne.g:\n\nBCO-DMO: Bianchi\n\nOutput from model simulations, which will include both physical (T, S, velocities) and biogeochemical (tracer, rates, fluxes) variables. The raw output will be post-processed and analyzed for presentation at meetings and for publication in scientific journals. For model output from the main set of simulations (Table 1 in the proposal), standard model fields (e.g. physical and biogeochemical tracers, current velocities, and the main biogeochemical rates) at monthly resolution will be provided to, and made available through, BCO-DMO and NOAA NCEI in standard NetCDF format.\n\n\n\n\nSamples\n\nPhysical samples (e.g., biological, chemical, geological)\n\ne.g.:\n\nMullineaux #2453 - Physical samples will include animals, rocks, and water column samples. Rocks and animal specimens for which we extract gut contents will be assigned unique sample identifiers, important for provenance for subsamples to be analyzed for microbes.\n\n\nDigital samples (e.g., scanned images, 3D models)\n\nSoftware\n\nCode, scripts, or applications developed for analysis\n\n\n\n\n\n\n\n\nGeneral example\n\n\n\nNote their description of data types, number of experiments performed, file types generated, and repositories for submission.\nDescription of data types, number of experiments performed, file types generated, and repositories for submission\n\nGenetic sequencing: mRNA and DNA sequencing from E. huxleyi cultures grown in the lab will be collected. Sequencing will be performed at the Columbia Genome Center (New York, NY). All raw sequence data will be deposited in the short-read archive (SRA) through the NCBI. Assembled genomes will be deposited under the same project number through the NCBI’s Assembly database. Assembled transcriptomes will be deposited on the NCBI’s Transcriptome Shotgun Assembly (TSA) database. Associated annotation files will be uploaded with the genome and transcriptome files to the appropriate database. File types: Short-read archive (.sra), raw sequencing files (.fastq), assembled fasta files (.fasta), annotation files (.gff). Repository: NCBI; accession numbers will be provided to BCO-DMO.\nPhysiological experiment data: Physiological experiments carried out on 10 different E. huxleyi strains will be conducted in the lab. Physiological and chemical parameters of the cultures will be collected over time including: growth rate, cell size, Fv/Fm, chlorophyll, particulate organic carbon, particulate inorganic carbon, dissolved nitrogen and phosphorus, and particulate nitrogen and phosphorus. File types: csv files. Repository: BCO-DMO.\nBioinformatic pipelines: Analysis of all genetic sequencing data will be done through the construction of reproducible pipelines designed in Snakemake. These pipelines will be made public on GitHub and archived and provided a doi through Zenodo. Repository: Zenodo; doi will be provided to BCO-DMO.\n\n\n\n\n\nNumber and Size of Research Outputs\n\nNumber of files, samples, collections, etc.\n\nEstimated count of each type of output.\n\n\n\nSize of each file, sample, etc.\n\nFile sizes (e.g., MB, GB, TB) for digital outputs.\n\nPhysical dimensions or quantities for physical samples.\n\n\n\n\n\n\n\n\nLTER: Sosik\n\n\n\nBy mid-year 6 of our project, our IM team stores ~60 TB of data (~55 on premises and ~5 cloud). For WHOI’s institutional research data storage (RDS) on premises, we have &gt;40 TB to share within WHOI a subset of our towed plankton imaging data, 12 TB for a subset of our coupled physical-biological model output, and 1 TB for (low-volume) data products including those served by our web-based REST API. However, additional storage is required for plankton imagery (e.g., PI Sosik’s network attached storage serving ~10 TB publicly available IFCB data), acoustic data collected by vessels and Stingray towed vehicle, and physical and coupled biological-physical model output; these and some other high-volume data types (e.g., high throughput sequencing data) are necessarily managed by PIs.\n\n\n\n\n\nMetadata Critical for Data Interpretation and Reuse\n\nSensor metadata\n\nInformation about instruments or sensors used to collect data.\n\n\n\n\n\n\n\nBCO–DMO: Church\n\n\n\nAll underway optical data including imaging flow cytometry will be time-synchronized with the ship’s GPS and thermosalinograph. At sea all data will be visualized in real time to evaluate data quality and instrument stability; data will be stored on a customized raspberry-PI based data logger and backed up onto a separate hard drive. Post-cruise, all continuous optical data will be merged to 1-min averages and submitted to BCO-DMO; Imaging flow cytometry data are sampled at ~20 min intervals; a composite cruise file of all imaged particles and their individual morphometrics and optical properties will be generated and submitted to BCO-DMO.\n\n\n\n\nSampling effort metadata\n\nDetails about sampling methods, locations, and conditions\n\n\n\n\n\n\n\nBCO-DMO: Reitzel\n\n\n\nSequence data will be annotated with essential information for each sample, which will include description of the sample (e.g., location of origin for population, date), method for library generation, and the sequencing method. These data tags for description will be included with data submitted to SRA and BCO-DMO. Nucleic acids extracted from collected animals will be labeled with precise site of origin (latitude, longitude), date of collection, and species and archived in the Reitzel and Burt laboratories. Field temperature data will be stored in flat ASCII files, which can be read easily by different software packages. Field data will include date, time, latitude, longitude, and temperature, as appropriate.",
    "crumbs": [
      "Research Outputs"
    ]
  },
  {
    "objectID": "sections/pids.html",
    "href": "sections/pids.html",
    "title": "Persistent Identifiers (PIDs)",
    "section": "",
    "text": "Use of PIDs (e.g., DOI, Handle, ARK) to uniquely identify outputs.",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/pids.html#research-outputs",
    "href": "sections/pids.html#research-outputs",
    "title": "Persistent Identifiers (PIDs)",
    "section": "",
    "text": "Use of PIDs (e.g., DOI, Handle, ARK) to uniquely identify outputs.",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/pids.html#formats-and-standards",
    "href": "sections/pids.html#formats-and-standards",
    "title": "Persistent Identifiers (PIDs)",
    "section": "Formats and Standards",
    "text": "Formats and Standards\n\nPIDs for metadata schemas and standards.",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/pids.html#access",
    "href": "sections/pids.html#access",
    "title": "Persistent Identifiers (PIDs)",
    "section": "Access",
    "text": "Access\n\nPIDs for access protocols and repositories.",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/pids.html#data-management",
    "href": "sections/pids.html#data-management",
    "title": "Persistent Identifiers (PIDs)",
    "section": "Data Management",
    "text": "Data Management\n\nPIDs for datasets and related documentation.\n\nExample PIDs:\n\nORCID - person PIDs\nROR - research organization PIDs\nRe3data - specifically PIDs for data repositories\nFunding Award Numbers\nRAID - project meta-PID\nRRID\nDataCite (DOIs)\n\n\n\n\n\n\n\nAdditional reading\n\n\n\nTen simple rules for getting and giving credit for data\n\nWood-Charlson EM, Crockett Z, Erdmann C, Arkin AP, Robinson CB (2022) Ten simple rules for getting and giving credit for data. PLOS Computational Biology 18(9): e1010476. https://doi.org/10.1371/journal.pcbi.1010476",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/access.html",
    "href": "sections/access.html",
    "title": "Access to Research Outputs",
    "section": "",
    "text": "Mechanisms for sharing outputs (e.g., repositories, cloud storage).\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nTo increase accessibility to project data and the dissemination of our research findings—particularly among scientists from developing countries—we will make every effort to publish our results as open-access articles or within open-access journals.\n\n\n\n\n\n\nWhere the data will be stored (e.g., institutional repository, public archive).\n\n\n\n\n\n\n\nBCO-DMO: Saito\n\n\n\nR2R will ensure that the original underway measurements are archived permanently at NCEI and/or NGDC as appropriate. BCO-DMO will also ensure that project data are submitted to the appropriate national data archive. The PI will work with R2R and BCO-DMO to ensure data are archived appropriately and that proper and complete documentation are archived along with the data. All processed data will be submitted to BCO-DMO. Sequence data will be submitted to NCBI and raw mass spectrometry data will be submitted to ProteomeXchange.\n\n\n\n\n\n\n\n\nBCO-DMO: Reitzel\n\n\n\nSequence data will be archived at NCBI GenBank or SRA, code will be archived on GitHub and processed data files (e.g., vcf files) will be deposited in Dryad. BCO-DMO ensures that the data are archived properly at the appropriate National Data Archive for long-term archive preservation.\n\n\n\n\n\n\nMeasures to protect sensitive or confidential data (e.g., encryption, access controls).\n\n\n\n\n\nWhen will products be shared or published (e.g., immediately, after embargo period).\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nAll of the data generated in this study will be made publicly available upon publication in a peer-reviewed journal, or within two years of the completion of the project. […] Shipboard underway data will be made available within one year via the Rolling Deck to Repository.\n\n\n\n\n\n\n\n\nBCO-DMO: Greer\n\n\n\nOnce the data have been collected and quality controlled, we will ensure all data are publicly available within two years. In general, we do not intend to impose any data embargos, with the exception of student generated data that will be used for the completion of student theses and publications.\n\n\n\n\n\n\nLicensing terms (e.g., Creative Commons, MIT License).\n\n\n\n\n\n\n\nBCO–DMO: Church\n\n\n\nAll data from this project are considered within the public domain for all not-for-profit uses and there will be no permission restrictions placed on use of the data.\nAuthors use right language but do not explicitly declare the license.\n\n\n\n\n\n\n\n\nMCR-LTER\n\n\n\nOur data access/data use policy is the Creative Commons license CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/). The user is free to use and adapt the data while giving appropriate credit without any other restrictions. In our data use policy, we make a few additional recommendations for collaboration and suggest that users contact the appropriate MCR personnel if they have any questions or concerns about the data, but these recommendations do not change the general license text.\n\n\n\n\n\n\nAny limitations on access due to confidentiality or proprietary concerns.\n\n\n\n\n\n\n\nBCO-DMO: Greer\n\n\n\nWe do not expect that the data we will generate will require any exceptional arrangements due to questions of ethical restrictions or release of indigenous knowledge.\n\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nThere are no ethical and privacy issues with these data. There are no human research subjects in our study. The dataset from this project will not be copyrighted.\nAuthors use right language but do not explicitly declare the license.",
    "crumbs": [
      "Access to Research Outputs"
    ]
  },
  {
    "objectID": "sections/access.html#what-does-access-look-like",
    "href": "sections/access.html#what-does-access-look-like",
    "title": "Access to Research Outputs",
    "section": "",
    "text": "Mechanisms for sharing outputs (e.g., repositories, cloud storage).\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nTo increase accessibility to project data and the dissemination of our research findings—particularly among scientists from developing countries—we will make every effort to publish our results as open-access articles or within open-access journals.\n\n\n\n\n\n\nWhere the data will be stored (e.g., institutional repository, public archive).\n\n\n\n\n\n\n\nBCO-DMO: Saito\n\n\n\nR2R will ensure that the original underway measurements are archived permanently at NCEI and/or NGDC as appropriate. BCO-DMO will also ensure that project data are submitted to the appropriate national data archive. The PI will work with R2R and BCO-DMO to ensure data are archived appropriately and that proper and complete documentation are archived along with the data. All processed data will be submitted to BCO-DMO. Sequence data will be submitted to NCBI and raw mass spectrometry data will be submitted to ProteomeXchange.\n\n\n\n\n\n\n\n\nBCO-DMO: Reitzel\n\n\n\nSequence data will be archived at NCBI GenBank or SRA, code will be archived on GitHub and processed data files (e.g., vcf files) will be deposited in Dryad. BCO-DMO ensures that the data are archived properly at the appropriate National Data Archive for long-term archive preservation.\n\n\n\n\n\n\nMeasures to protect sensitive or confidential data (e.g., encryption, access controls).\n\n\n\n\n\nWhen will products be shared or published (e.g., immediately, after embargo period).\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nAll of the data generated in this study will be made publicly available upon publication in a peer-reviewed journal, or within two years of the completion of the project. […] Shipboard underway data will be made available within one year via the Rolling Deck to Repository.\n\n\n\n\n\n\n\n\nBCO-DMO: Greer\n\n\n\nOnce the data have been collected and quality controlled, we will ensure all data are publicly available within two years. In general, we do not intend to impose any data embargos, with the exception of student generated data that will be used for the completion of student theses and publications.\n\n\n\n\n\n\nLicensing terms (e.g., Creative Commons, MIT License).\n\n\n\n\n\n\n\nBCO–DMO: Church\n\n\n\nAll data from this project are considered within the public domain for all not-for-profit uses and there will be no permission restrictions placed on use of the data.\nAuthors use right language but do not explicitly declare the license.\n\n\n\n\n\n\n\n\nMCR-LTER\n\n\n\nOur data access/data use policy is the Creative Commons license CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/). The user is free to use and adapt the data while giving appropriate credit without any other restrictions. In our data use policy, we make a few additional recommendations for collaboration and suggest that users contact the appropriate MCR personnel if they have any questions or concerns about the data, but these recommendations do not change the general license text.\n\n\n\n\n\n\nAny limitations on access due to confidentiality or proprietary concerns.\n\n\n\n\n\n\n\nBCO-DMO: Greer\n\n\n\nWe do not expect that the data we will generate will require any exceptional arrangements due to questions of ethical restrictions or release of indigenous knowledge.\n\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nThere are no ethical and privacy issues with these data. There are no human research subjects in our study. The dataset from this project will not be copyrighted.\nAuthors use right language but do not explicitly declare the license.",
    "crumbs": [
      "Access to Research Outputs"
    ]
  },
  {
    "objectID": "sections/roles_responsibilities.html",
    "href": "sections/roles_responsibilities.html",
    "title": "Roles and Responsibilities",
    "section": "",
    "text": "Ensures the DMP is followed by the research team.\n\ne.g: Mullineaux #2453:\n\nLead PI will ensure compliance to this data management plan.\n\n\n\n\n\n\n\nOversees data management practices.\nEnsures compliance with the Data Management Plan (DMP).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\n\nObservations at seafloor: All PIs\nImagery at seafloor - PI3 and PI4\nPhysical sample cataloging:\n\nPI1 will obtain IGSNs for rock samples; PI2 will provide unique\n\nAnimal specimen identifiers:\n\nEach PI will catalog his/her respective subsamples associated to identifier, when applicable.\n\nData for animals - PI-3 and PI-4\nData for microbes - PI-1 and PI-2\nData for minerals - PI-3\nDerived data from integrating: All PIs\n\n\n\n\n\n\n\nPrimary individual responsible for DMP updates and communication.\nEnsures the DMP is followed by the research team.\n\n\n\n\n\n\n\nBCo-DMO: Mullineaux #2453:\n\n\n\n\nLead PI will ensure compliance to this data management plan.\nDetails of external parties involved in data management.\n\n\n\n\nServes as the primary communication person for the DMP\nPrimary individual responsible for DMP updates.\n\n\n\n\n\nExternal party that contributes to the project",
    "crumbs": [
      "Roles and Responsibilities"
    ]
  },
  {
    "objectID": "sections/roles_responsibilities.html#principle-investigator",
    "href": "sections/roles_responsibilities.html#principle-investigator",
    "title": "Roles and Responsibilities",
    "section": "",
    "text": "Ensures the DMP is followed by the research team.\n\ne.g: Mullineaux #2453:\n\nLead PI will ensure compliance to this data management plan.",
    "crumbs": [
      "Roles and Responsibilities"
    ]
  },
  {
    "objectID": "sections/roles_responsibilities.html#data-manager",
    "href": "sections/roles_responsibilities.html#data-manager",
    "title": "Roles and Responsibilities",
    "section": "",
    "text": "Oversees data management practices.\nEnsures compliance with the Data Management Plan (DMP).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\n\nObservations at seafloor: All PIs\nImagery at seafloor - PI3 and PI4\nPhysical sample cataloging:\n\nPI1 will obtain IGSNs for rock samples; PI2 will provide unique\n\nAnimal specimen identifiers:\n\nEach PI will catalog his/her respective subsamples associated to identifier, when applicable.\n\nData for animals - PI-3 and PI-4\nData for microbes - PI-1 and PI-2\nData for minerals - PI-3\nDerived data from integrating: All PIs",
    "crumbs": [
      "Roles and Responsibilities"
    ]
  },
  {
    "objectID": "sections/roles_responsibilities.html#point-of-contact-for-the-dmp",
    "href": "sections/roles_responsibilities.html#point-of-contact-for-the-dmp",
    "title": "Roles and Responsibilities",
    "section": "",
    "text": "Primary individual responsible for DMP updates and communication.\nEnsures the DMP is followed by the research team.\n\n\n\n\n\n\n\nBCo-DMO: Mullineaux #2453:\n\n\n\n\nLead PI will ensure compliance to this data management plan.\nDetails of external parties involved in data management.\n\n\n\n\nServes as the primary communication person for the DMP\nPrimary individual responsible for DMP updates.",
    "crumbs": [
      "Roles and Responsibilities"
    ]
  },
  {
    "objectID": "sections/roles_responsibilities.html#third-partycontractorconsultant",
    "href": "sections/roles_responsibilities.html#third-partycontractorconsultant",
    "title": "Roles and Responsibilities",
    "section": "",
    "text": "External party that contributes to the project",
    "crumbs": [
      "Roles and Responsibilities"
    ]
  },
  {
    "objectID": "sections/data_management.html",
    "href": "sections/data_management.html",
    "title": "Data Management",
    "section": "",
    "text": "Comprehensive plan for managing research data throughout its lifecycle.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "sections/data_management.html#overview",
    "href": "sections/data_management.html#overview",
    "title": "Data Management",
    "section": "",
    "text": "Comprehensive plan for managing research data throughout its lifecycle.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "sections/data_management.html#key-components",
    "href": "sections/data_management.html#key-components",
    "title": "Data Management",
    "section": "Key Components",
    "text": "Key Components\n\nData Collection\n\nMethods and tools for data collection.\n\nData Storage\n\nSecure and scalable storage solutions.\n\nData Backup\n\nRegular backups to prevent data loss.\n\nData Archiving\n\nLong-term preservation of data in trusted repositories.\n\nData Sharing\n\nPolicies and platforms for sharing data with the broader community.\n\nData Retention\n\nGuidelines for retaining or disposing of data after project completion.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "esipDMP Primer",
    "section": "",
    "text": "This website is actively in construction. Please check back soon for more information.",
    "crumbs": [
      "Welcome"
    ]
  }
]