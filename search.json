[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "sections/standardization.html",
    "href": "sections/standardization.html",
    "title": "Standardization of Data and Metadata",
    "section": "",
    "text": "Describe the formats (e.g. ASCII, NetCDF), and standards (e.g., Darwin Core) of data collected during this project. Describe the types (e.g., sensor metadata, data file metadata, project metadata, tag metadata), formats (e.g., JSON, XML), and standards (e.g., EML, ISO) of metadata collected during this project. Also describe any methodological or QAQC standards that will be used in processing data and samples generated by this project. \n\n\n\nUse of widely accepted formats (e.g., CSV, JSON, HDF5 for data; XML, JSON-LD for metadata).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nField observation data will be stored in flat ASCII files, which can be read easily by different software packages. Observational and experimental data from processed samples will be stored mainly as spreadsheets. Imagery from the seafloor will be stored in original resolution (this will be the biggest data volume in this project). Metadata will be prepared in accordance with BCO-DMO conventions (i.e. using the BCO-DMO metadata forms) and will include detailed descriptions of collection and analysis procedures.\n\n\n\n\n\n\nAdherence to domain-specific standards (e.g., Darwin Core for biodiversity data, DICOM for medical imaging).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nSpecifically for animals: We will utilize the World Register of Marine Species (WoRMS) taxonomic classification. We will standardize to Darwin Core format to provide occurrence data to OBIS and GBIF. Specifically for microbes: Genetic sequence data will be prepared in accordance with the minimum information about a marker gene sequence (MIMARKS) and about a metagenome sequence (MIMS) developed by the Genomic Standards Consortium, with quality control screening of raw reads from 16S/18S rRNA sequencing and raw metagenomic sequences. Specifically for minerals: Rock samples will be assigned International Geo Sample Numbers (IGSN) through SESAR (System for Earth Sample Registration)\n\n\n\nCompliance with FAIR principles (Findable, Accessible, Interoperable, Reusable).\n\n\n\n\n\n\n\nLTER: Sosik\n\n\n\nTo contribute FAIR (Findable, Accessible, Interoperable, Reusable) data products to DataONE and other community repositories, our IM team uses non-proprietary data formats, standardizes metadata, and promotes the use of controlled vocabularies.",
    "crumbs": [
      "Standardization of Data and Metadata"
    ]
  },
  {
    "objectID": "sections/standardization.html#naming-conventions-and-file-structures",
    "href": "sections/standardization.html#naming-conventions-and-file-structures",
    "title": "Standardization of Data and Metadata",
    "section": "",
    "text": "Describe the formats (e.g. ASCII, NetCDF), and standards (e.g., Darwin Core) of data collected during this project. Describe the types (e.g., sensor metadata, data file metadata, project metadata, tag metadata), formats (e.g., JSON, XML), and standards (e.g., EML, ISO) of metadata collected during this project. Also describe any methodological or QAQC standards that will be used in processing data and samples generated by this project. \n\n\n\nUse of widely accepted formats (e.g., CSV, JSON, HDF5 for data; XML, JSON-LD for metadata).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nField observation data will be stored in flat ASCII files, which can be read easily by different software packages. Observational and experimental data from processed samples will be stored mainly as spreadsheets. Imagery from the seafloor will be stored in original resolution (this will be the biggest data volume in this project). Metadata will be prepared in accordance with BCO-DMO conventions (i.e. using the BCO-DMO metadata forms) and will include detailed descriptions of collection and analysis procedures.\n\n\n\n\n\n\nAdherence to domain-specific standards (e.g., Darwin Core for biodiversity data, DICOM for medical imaging).\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nSpecifically for animals: We will utilize the World Register of Marine Species (WoRMS) taxonomic classification. We will standardize to Darwin Core format to provide occurrence data to OBIS and GBIF. Specifically for microbes: Genetic sequence data will be prepared in accordance with the minimum information about a marker gene sequence (MIMARKS) and about a metagenome sequence (MIMS) developed by the Genomic Standards Consortium, with quality control screening of raw reads from 16S/18S rRNA sequencing and raw metagenomic sequences. Specifically for minerals: Rock samples will be assigned International Geo Sample Numbers (IGSN) through SESAR (System for Earth Sample Registration)\n\n\n\nCompliance with FAIR principles (Findable, Accessible, Interoperable, Reusable).\n\n\n\n\n\n\n\nLTER: Sosik\n\n\n\nTo contribute FAIR (Findable, Accessible, Interoperable, Reusable) data products to DataONE and other community repositories, our IM team uses non-proprietary data formats, standardizes metadata, and promotes the use of controlled vocabularies.",
    "crumbs": [
      "Standardization of Data and Metadata"
    ]
  },
  {
    "objectID": "sections/standardization.html#qaqc-methods",
    "href": "sections/standardization.html#qaqc-methods",
    "title": "Standardization of Data and Metadata",
    "section": "QA/QC Methods",
    "text": "QA/QC Methods\n\nImplementation of quality assurance and quality control processes.\nRegular audits and validation checks to ensure data integrity.\n\n\n\n\n\n\n\nBCO-DMO: Saito\n\n\n\nQuality flags will be assigned according to the ODS IODE Quality Flag scheme (IOC Manuals and Guides, 54, volume 3; http://www.iode.org/mg54_3.\n\n\n\n\n\n\n\n\nIOOS: SCCOOS\n\n\n\nFor sources that do not provide quality flags, the SCCOOS DMAC Sub-System runs QARTOD tests after ingesting observation data. Tests are run using the open-source ioos_qc library, which implements a suite of QARTOD tests as well as other quality control algorithms. The quality test code and test thresholds are documented and publicly available through the CalOOS Data Portal. Links to the ioos_qc methods used are available both within data charts and on sensor pages within the CalOOS Data Portal. Thresholds used for each test are also viewable on sensor pages and users are linked to the test code in GitHub.",
    "crumbs": [
      "Standardization of Data and Metadata"
    ]
  },
  {
    "objectID": "sections/outputs.html",
    "href": "sections/outputs.html",
    "title": "Research Outputs",
    "section": "",
    "text": "Research outputs include a variety of data and materials generated during the research process. Given the planned project and data collection, provide the most accurate estimate possible of the types of research outputs. This information will help calculate data/sample storage needs, and may help determine where the data can be published or archived. \n\n\n\nCurated sets of samples or data\n\n\n\n\n\nRaw data\nProcessed data\nMetadata\n\n\n\n\n\nResearch papers, conference posters, pre-prints, and multimedia content\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nImagery at the seafloor will include NDSF vehicle imagery (e.g., 4K video) and PI-provided imagery (e.g., time-lapse still).\n\n\n\n\n\n\nLab protocols, reagents, or experimental setups\n\n\n\n\n\nStatistical, mathematical, or computational models\n\n\n\n\n\n\n\nBCO-DMO: Bianchi\n\n\n\nOutput from model simulations, which will include both physical (T, S, velocities) and biogeochemical (tracer, rates, fluxes) variables. The raw output will be post-processed and analyzed for presentation at meetings and for publication in scientific journals. For model output from the main set of simulations (Table 1 in the proposal), standard model fields (e.g. physical and biogeochemical tracers, current velocities, and the main biogeochemical rates) at monthly resolution will be provided to, and made available through, BCO-DMO and NOAA NCEI in standard NetCDF format.\n\n\n\n\n\n\nPhysical samples (e.g., biological, chemical, geological)\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nPhysical samples will include animals, rocks, and water column samples. Rocks and animal specimens for which we extract gut contents will be assigned unique sample identifiers, important for provenance for subsamples to be analyzed for microbes.\n\n\n\nDigital samples (e.g., scanned images, 3D models)\n\n\n\n\n\nCode, scripts, or applications developed for analysis",
    "crumbs": [
      "Research Outputs"
    ]
  },
  {
    "objectID": "sections/outputs.html#type-of-research-output",
    "href": "sections/outputs.html#type-of-research-output",
    "title": "Research Outputs",
    "section": "",
    "text": "Research outputs include a variety of data and materials generated during the research process. Given the planned project and data collection, provide the most accurate estimate possible of the types of research outputs. This information will help calculate data/sample storage needs, and may help determine where the data can be published or archived. \n\n\n\nCurated sets of samples or data\n\n\n\n\n\nRaw data\nProcessed data\nMetadata\n\n\n\n\n\nResearch papers, conference posters, pre-prints, and multimedia content\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nImagery at the seafloor will include NDSF vehicle imagery (e.g., 4K video) and PI-provided imagery (e.g., time-lapse still).\n\n\n\n\n\n\nLab protocols, reagents, or experimental setups\n\n\n\n\n\nStatistical, mathematical, or computational models\n\n\n\n\n\n\n\nBCO-DMO: Bianchi\n\n\n\nOutput from model simulations, which will include both physical (T, S, velocities) and biogeochemical (tracer, rates, fluxes) variables. The raw output will be post-processed and analyzed for presentation at meetings and for publication in scientific journals. For model output from the main set of simulations (Table 1 in the proposal), standard model fields (e.g. physical and biogeochemical tracers, current velocities, and the main biogeochemical rates) at monthly resolution will be provided to, and made available through, BCO-DMO and NOAA NCEI in standard NetCDF format.\n\n\n\n\n\n\nPhysical samples (e.g., biological, chemical, geological)\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\nPhysical samples will include animals, rocks, and water column samples. Rocks and animal specimens for which we extract gut contents will be assigned unique sample identifiers, important for provenance for subsamples to be analyzed for microbes.\n\n\n\nDigital samples (e.g., scanned images, 3D models)\n\n\n\n\n\nCode, scripts, or applications developed for analysis",
    "crumbs": [
      "Research Outputs"
    ]
  },
  {
    "objectID": "sections/outputs.html#number-and-size-of-research-outputs",
    "href": "sections/outputs.html#number-and-size-of-research-outputs",
    "title": "Research Outputs",
    "section": "Number and Size of Research Outputs",
    "text": "Number and Size of Research Outputs\nGiven the planned project and data collection, provide the most accurate estimate possible of the types of research outputs. This information will help calculate data/sample storage needs, and may help determine where the data can be published or archived. \n\nNumber of files, samples, collections, etc.\n\nEstimated count of each type of output.\n\n\n\nSize of each file, sample, etc.\n\nFile sizes (e.g., MB, GB, TB) for digital outputs.\n\nPhysical dimensions or quantities for physical samples.\n\n\n\n\n\n\n\n\nLTER: Sosik\n\n\n\nBy mid-year 6 of our project, our IM team stores ~60 TB of data (~55 on premises and ~5 cloud). For WHOI’s institutional research data storage (RDS) on premises, we have &gt;40 TB to share within WHOI a subset of our towed plankton imaging data, 12 TB for a subset of our coupled physical-biological model output, and 1 TB for (low-volume) data products including those served by our web-based REST API. However, additional storage is required for plankton imagery (e.g., PI Sosik’s network attached storage serving ~10 TB publicly available IFCB data), acoustic data collected by vessels and Stingray towed vehicle, and physical and coupled biological-physical model output; these and some other high-volume data types (e.g., high throughput sequencing data) are necessarily managed by PIs.\n\n\n\n\n\n\n\n\nGeneral example\n\n\n\nNote their description of data types, number of experiments performed, file types generated, and repositories for submission.\nDescription of data types, number of experiments performed, file types generated, and repositories for submission\n\nGenetic sequencing: mRNA and DNA sequencing from E. huxleyi cultures grown in the lab will be collected. Sequencing will be performed at the Columbia Genome Center (New York, NY). All raw sequence data will be deposited in the short-read archive (SRA) through the NCBI. Assembled genomes will be deposited under the same project number through the NCBI’s Assembly database. Assembled transcriptomes will be deposited on the NCBI’s Transcriptome Shotgun Assembly (TSA) database. Associated annotation files will be uploaded with the genome and transcriptome files to the appropriate database. File types: Short-read archive (.sra), raw sequencing files (.fastq), assembled fasta files (.fasta), annotation files (.gff). Repository: NCBI; accession numbers will be provided to BCO-DMO.\nPhysiological experiment data: Physiological experiments carried out on 10 different E. huxleyi strains will be conducted in the lab. Physiological and chemical parameters of the cultures will be collected over time including: growth rate, cell size, Fv/Fm, chlorophyll, particulate organic carbon, particulate inorganic carbon, dissolved nitrogen and phosphorus, and particulate nitrogen and phosphorus. File types: csv files. Repository: BCO-DMO.\nBioinformatic pipelines: Analysis of all genetic sequencing data will be done through the construction of reproducible pipelines designed in Snakemake. These pipelines will be made public on GitHub and archived and provided a doi through Zenodo. Repository: Zenodo; doi will be provided to BCO-DMO.",
    "crumbs": [
      "Research Outputs"
    ]
  },
  {
    "objectID": "sections/outputs.html#metadata-critical-for-data-interpretation-and-reuse",
    "href": "sections/outputs.html#metadata-critical-for-data-interpretation-and-reuse",
    "title": "Research Outputs",
    "section": "Metadata Critical for Data Interpretation and Reuse",
    "text": "Metadata Critical for Data Interpretation and Reuse\nDescribe the accompanying information that is needed to interpret the data, such as effort or calibration information. Describe what other data will be shared to enable interpretation of the data. This may include metadata (e.g., about sensors or other equipment), temporal and spatial details, or effort data (e.g., from observational surveys). \n\nSensor / instrument metadata\n\nInformation about instruments or sensors used to collect data.\n\n\n\n\n\n\n\nBCO–DMO: Church\n\n\n\nAll underway optical data including imaging flow cytometry will be time-synchronized with the ship’s GPS and thermosalinograph. At sea all data will be visualized in real time to evaluate data quality and instrument stability; data will be stored on a customized raspberry-PI based data logger and backed up onto a separate hard drive. Post-cruise, all continuous optical data will be merged to 1-min averages and submitted to BCO-DMO; Imaging flow cytometry data are sampled at ~20 min intervals; a composite cruise file of all imaged particles and their individual morphometrics and optical properties will be generated and submitted to BCO-DMO.\n\n\n\n\nSampling effort metadata\n\nDetails about sampling methods, locations, and conditions\n\n\n\n\n\n\n\nBCO-DMO: Reitzel\n\n\n\nSequence data will be annotated with essential information for each sample, which will include description of the sample (e.g., location of origin for population, date), method for library generation, and the sequencing method. These data tags for description will be included with data submitted to SRA and BCO-DMO. Nucleic acids extracted from collected animals will be labeled with precise site of origin (latitude, longitude), date of collection, and species and archived in the Reitzel and Burt laboratories. Field temperature data will be stored in flat ASCII files, which can be read easily by different software packages. Field data will include date, time, latitude, longitude, and temperature, as appropriate.",
    "crumbs": [
      "Research Outputs"
    ]
  },
  {
    "objectID": "sections/pids.html",
    "href": "sections/pids.html",
    "title": "Persistent Identifiers (PIDs)",
    "section": "",
    "text": "Use of PIDs (e.g., DOI, Handle, ARK) to uniquely identify outputs.",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/pids.html#research-outputs",
    "href": "sections/pids.html#research-outputs",
    "title": "Persistent Identifiers (PIDs)",
    "section": "",
    "text": "Use of PIDs (e.g., DOI, Handle, ARK) to uniquely identify outputs.",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/pids.html#formats-and-standards",
    "href": "sections/pids.html#formats-and-standards",
    "title": "Persistent Identifiers (PIDs)",
    "section": "Formats and Standards",
    "text": "Formats and Standards\n\nPIDs for metadata schemas and standards.",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/pids.html#access",
    "href": "sections/pids.html#access",
    "title": "Persistent Identifiers (PIDs)",
    "section": "Access",
    "text": "Access\n\nPIDs for access protocols and repositories.",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/pids.html#data-management",
    "href": "sections/pids.html#data-management",
    "title": "Persistent Identifiers (PIDs)",
    "section": "Data Management",
    "text": "Data Management\n\nPIDs for datasets and related documentation.\n\nExample PIDs:\n\nORCID - person PIDs\nROR - research organization PIDs\nRe3data - specifically PIDs for data repositories\nFunding Award Numbers\nRAID - project meta-PID\nRRID\nDataCite (DOIs)\n\n\n\n\n\n\n\nAdditional reading\n\n\n\nTen simple rules for getting and giving credit for data\n\nWood-Charlson EM, Crockett Z, Erdmann C, Arkin AP, Robinson CB (2022) Ten simple rules for getting and giving credit for data. PLOS Computational Biology 18(9): e1010476. https://doi.org/10.1371/journal.pcbi.1010476",
    "crumbs": [
      "Persistent Identifiers (PIDs)"
    ]
  },
  {
    "objectID": "sections/access.html",
    "href": "sections/access.html",
    "title": "Access to Research Outputs",
    "section": "",
    "text": "Access\nDescribe how access to the research outputs will be provided (e.g., by publishing with a public domain license in a robust repository). \n\n\nSharing\nDescribe any requirements to share the data from the project. Such requirements might come from a funder, participating organization, government agency, or other relevant entity. \n\nMechanisms for sharing outputs (e.g., repositories, cloud storage).\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nTo increase accessibility to project data and the dissemination of our research findings—particularly among scientists from developing countries—we will make every effort to publish our results as open-access articles or within open-access journals.\n\n\n\n\nArchive(s)\nDescribe the steps your project team will take to secure long-term availability of project data, and research outputs. This might include publishing data in robust repositories with strong sustainability plans, archiving back-up copies of data in long-term institutional cloud storage, including data maintenance in future funding requests and institutional budgets, and creating a long-term plan to ensure data remain available as staff change and data needs change. \n\nWhere the data will be stored (e.g., institutional repository, public archive).\n\n\n\n\n\n\n\nBCO-DMO: Saito\n\n\n\nR2R will ensure that the original underway measurements are archived permanently at NCEI and/or NGDC as appropriate. BCO-DMO will also ensure that project data are submitted to the appropriate national data archive. The PI will work with R2R and BCO-DMO to ensure data are archived appropriately and that proper and complete documentation are archived along with the data. All processed data will be submitted to BCO-DMO. Sequence data will be submitted to NCBI and raw mass spectrometry data will be submitted to ProteomeXchange.\n\n\n\n\n\n\n\n\nBCO-DMO: Reitzel\n\n\n\nSequence data will be archived at NCBI GenBank or SRA, code will be archived on GitHub and processed data files (e.g., vcf files) will be deposited in Dryad. BCO-DMO ensures that the data are archived properly at the appropriate National Data Archive for long-term archive preservation.\n\n\n\n\nTimeline for Availability/Access\nDescribe any timelines for making data available that are mandated (e.g., by a funder) or planned by the project leads. \n\nWhen will products be shared or published (e.g., immediately, after embargo period).\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nAll of the data generated in this study will be made publicly available upon publication in a peer-reviewed journal, or within two years of the completion of the project. […] Shipboard underway data will be made available within one year via the Rolling Deck to Repository.\n\n\n\n\n\n\n\n\nBCO-DMO: Greer\n\n\n\nOnce the data have been collected and quality controlled, we will ensure all data are publicly available within two years. In general, we do not intend to impose any data embargos, with the exception of student generated data that will be used for the completion of student theses and publications.\n\n\n\n\nLicense\nDescribe any licensing requirements from funders, participating organizations, or government agencies, including a species license (e.g. CC-BY) or data use agreement. \n\nLicensing terms (e.g., Creative Commons, MIT License).\n\n\n\n\n\n\n\nBCO–DMO: Church\n\n\n\nAll data from this project are considered within the public domain for all not-for-profit uses and there will be no permission restrictions placed on use of the data.\nAuthors use right language but do not explicitly declare the license.\n\n\n\n\n\n\n\n\nMCR-LTER\n\n\n\nOur data access/data use policy is the Creative Commons license CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/). The user is free to use and adapt the data while giving appropriate credit without any other restrictions. In our data use policy, we make a few additional recommendations for collaboration and suggest that users contact the appropriate MCR personnel if they have any questions or concerns about the data, but these recommendations do not change the general license text.\n\n\n\n\nRestrictions / Confidentiality / Proprietary Business Info / Protection provisions\nDescribe any restrictions, embargoes, or other methods that will be applied to the data and research outputs, the methods for applying them, and for how long they will be applied. \n\nAny limitations on access due to confidentiality or proprietary concerns.\n\n\n\n\n\n\n\nBCO-DMO: Greer\n\n\n\nWe do not expect that the data we will generate will require any exceptional arrangements due to questions of ethical restrictions or release of indigenous knowledge.\n\n\n\n\n\n\n\n\nBCO-DMO: Church\n\n\n\nThere are no ethical and privacy issues with these data. There are no human research subjects in our study. The dataset from this project will not be copyrighted.\nAuthors use right language but do not explicitly declare the license.",
    "crumbs": [
      "Access to Research Outputs"
    ]
  },
  {
    "objectID": "sections/roles_responsibilities.html",
    "href": "sections/roles_responsibilities.html",
    "title": "Roles and Responsibilities",
    "section": "",
    "text": "Thoughtfully deciding who will fill the roles and have the responsibilities for data sharing and stewarding earth science data is critical. While all persons are expected to practice good data stewardship, this section asks for the contact info for the key project personnel [name, ORCID, affiliation, ROR, contact(email, phone)]. In addition, this section asks what plans are in place for handling turnover in personnel while maintaining the key roles and responsibilities.",
    "crumbs": [
      "Roles and Responsibilities"
    ]
  },
  {
    "objectID": "sections/roles_responsibilities.html#personnel",
    "href": "sections/roles_responsibilities.html#personnel",
    "title": "Roles and Responsibilities",
    "section": "Personnel",
    "text": "Personnel\nThe key roles are included below: Principal Investigator, Data Manager, Point of Contact for the DMP. Other roles that could be included here include: Data Provider, Data Collector, Data Analyst, Data Engineer, GIS Analyst, Contractor, Consultant.\n\nPrinciple Investigator\n\nEnsures the DMP is followed by the research team.\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453 Lead PI will ensure compliance to this data management plan.\n\n\n\n\n\n\n\n\nData Manager\n\nOversees data management practices.\nMay ensure compliance with the Data Management Plan (DMP).\nMay be the primary individual responsible for DMP updates.\n\n\n\n\n\n\n\nBCO-DMO: Mullineaux #2453\n\n\n\n\nObservations at seafloor: All PIs\nImagery at seafloor - PI3 and PI4\nPhysical sample cataloging:\n\nPI1 will obtain IGSNs for rock samples; PI2 will provide unique\n\nAnimal specimen identifiers:\n\nEach PI will catalog his/her respective subsamples associated to identifier, when applicable.\n\nData for animals - PI-3 and PI-4\nData for microbes - PI-1 and PI-2\nData for minerals - PI-3\nDerived data from integrating: All PIs\n\n\n\n\n\nPoint of Contact for the DMP\n\nPrimary individual responsible for DMP updates and communication.\nMay ensure the DMP is followed by the research team.\n\n\n\n\n\n\n\nBCo-DMO: Mullineaux #2453:\n\n\n\n\nLead PI will ensure compliance to this data management plan.\nDetails of external parties involved in data management.\n\n\n\n\nServes as the primary communication person for the DMP\nMay be the primary individual responsible for DMP updates.\n\n\n\nThird Party/Contractor/Consultant\n\nExternal party that contributes to the project",
    "crumbs": [
      "Roles and Responsibilities"
    ]
  },
  {
    "objectID": "sections/roles_responsibilities.html#plan-for-personnel-turnovertransfer",
    "href": "sections/roles_responsibilities.html#plan-for-personnel-turnovertransfer",
    "title": "Roles and Responsibilities",
    "section": "Plan for Personnel Turnover/Transfer",
    "text": "Plan for Personnel Turnover/Transfer\n\nProcedures for transferring responsibilities in case of personnel changes.\nDocumentation of roles and access permissions.",
    "crumbs": [
      "Roles and Responsibilities"
    ]
  },
  {
    "objectID": "sections/data_management.html",
    "href": "sections/data_management.html",
    "title": "Data Management",
    "section": "",
    "text": "Comprehensive plan for managing research data throughout its lifecycle.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "sections/data_management.html#overview",
    "href": "sections/data_management.html#overview",
    "title": "Data Management",
    "section": "",
    "text": "Comprehensive plan for managing research data throughout its lifecycle.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "sections/data_management.html#key-components",
    "href": "sections/data_management.html#key-components",
    "title": "Data Management",
    "section": "Key Components",
    "text": "Key Components\n\nData Collection\n\nMethods and tools for data collection.\n\nData Storage\n\nSecure and scalable storage solutions.\n\nData Backup\n\nRegular backups to prevent data loss.\n\nData Archiving\n\nLong-term preservation of data in trusted repositories.\n\nData Sharing\n\nPolicies and platforms for sharing data with the broader community.\n\nData Retention\n\nGuidelines for retaining or disposing of data after project completion.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Management: Planning for success",
    "section": "",
    "text": "Warning\n\n\n\nDisclaimer: This resource is currently under development, and we welcome your feedback and suggestions! (see below on how to submit feedback)\n\n\nGood data management requires planning, and most funding agencies require structured, comprehensive details describing how research outputs, like data, software, samples, etc. will be collected, stored, shared, and archived. Otherwise known as “DMPs”, or Data Management Plans. Note that different agencies may have slightly different acronyms for DMPs (see acronym list for more examples).\nThis resource provides some examples of DMPs from ESIP-relevant funding agencies (listed below), with the goal of providing:\n\nan overview of the sections/topics that should be covered in a DMP,\na brief description of best practices for those sections/topics, as generated during an ESIP 2025 Winter session, and\nexamples of DMPs from proposals funded by ESIP-relevant funding agencies.\n\nESIP-relevant funding agencies represented (so far) include:\n\nNational Science Foundation (NSF)\n\nArctic Data Center (ADC)\n\nBiological and Chemical Oceanography Data Management Office (BCO-DMO)\n\nLong Term Ecological Research (LTER)\n\n\nNational Aeronautics and Space Administration (NASA\n\nNational Oceanic and Atmospheric Administration (NOAA)\n\nU.S. Integrated Ocean Observing System (IOOS)\n\n\nOne resource to help with your DMP generation is the DMPTool, provided by the California Digital Library. DMPTool provides templates from a number of agencies, including those listed above, examples of published DMPs, and allows you to request a DOI for your DMP (highly recommended!). DMPTool is also working on creating machine-actionable DMPs (ma-DMPs), which will greatly improve the ability of funders and publishers to better track the impact of the research outputs from your science, as described in your DMP! Stay tuned for exciting advancements in this area, and read here for more information.\nThis resource was created as a working group partnership between the ESIP Marine Data Cluster and the ESIP Data Stewardship Committee. We are grateful to our ESIP Winter 2025 session speakers and participants for your help generating the best practices and ideas captured in this resource. We would also like to thank the Research Data Alliance (RDA) DMP Common Standards Working Group for their guidance and continued efforts to advance DMPs.\nThis resource leverages work currently being evolved by the DMP Common Standard. All sections with relevant DMP Common Standard components are linked to their GitHub definitions.\n\n\nHow to submit feedback / suggestions via GitHub issues\nNavigate to our issues page and open an issue related to your feedback. This requires an active GitHub account.\n\nTypes of issues include:\n\nNon-trivial revision - larger revisions to content or concepts\nSmall problem - typos, broken links\nSuggest new content - what are we missing?!\nDirect message to the team - any issues that you think should only be seen by team members.",
    "crumbs": [
      "Welcome"
    ]
  }
]